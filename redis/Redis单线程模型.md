# Redis单线程模型

Redis 基于 **Reactor 模式**开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）。文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。

当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。

**虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字**，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。

> I/O多路复用：一个进程维护多个Socket



## Redis没有使用多线程？为什么不用多线程？

1. 使用单线程模型能带来更好的可维护性，方便开发和调试；
2. 使用单线程模型也能并发的处理客户端的请求；
   - 使用 I/O 多路复用机制**并发**处理来自客户端的多个连接
3. **Redis 服务中运行的绝大多数操作的性能瓶颈都不是 CPU，而是在内存和网络**
   - 多线程技术能够帮助我们充分利用CPU资源来并发执行不同的任务，但CPU往往不是Redis的性能瓶颈
   - using pipelining Redis running on an average Linux system can deliver even 1 million requests per second, so if your application mainly **uses O(N) or O(log(N)) commands**, it is hardly going to use too much CPU.
   - 如果这种吞吐量不能满足要求的话，最好的做法就是上集群+分片，将不同请求交给不同Redis服务器来处理

**Redis6.0引入了多线程**

- 主要为了提高网络IO读写能力，因为这个算是 Redis 中的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络）。

  - 多线程默认是禁用的

  - 开启多线程后，还需要设置线程数，否则是不生效的。

  - ```bash
    io-threads-do-reads yes
    io-threads 4 #官网建议4核的机器建议设置为2或3个线程，8核的建议设置为6个线程
    ```

- 对于大键的删除，Redis可能会需要在释放内存空间上消耗更多时间，这就会阻塞待处理的任务，然而释放内存空间可以在后台异步处理，引入多线程的另一目的就是对一个大键值对的删除



## 文件事件处理器的四个部分

- 多个socket（客户端连接）
- I/O多路复用程序（支持多个客户端连接的关键）
- 文件事件分派器（将socket关联到相应的事件处理器）
- 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）
- 连接多个套接字，文件事件可能并发出现，但I/O多路复用程序会将套接字放在一个队列里面有序、同步、每次一个套接字的方式向文件事件分派器传送套接字。

![image-20220225093051308](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225093051308.png)



## I/O 多路复用：select/poll/epoll

### 最基本的 Socket 模型

客户端和服务器能在网络中通信，那必须得使⽤ Socket 编程，它是进程间通信⾥⽐较特别的⽅式，特别之处在于它是可以跨主机间通信。

1、创建 Socket 的时候，可以指定⽹络层使⽤的是 IPv4 还是 IPv6，传输层使⽤的是 TCP 还是 UDP。（这里看TCP的）

2、服务端⾸先调⽤ **socket()** 函数，创建⽹络协议为 IPv4，以及传输协议为 TCP 的 Socket ，接着调⽤**bind()** 函数，给这个 Socket 绑定⼀个 IP 地址和端⼝（绑的是自己的IP和监听的端口）

- 绑定端口的目的：当内核收到 TCP 报⽂，通过 TCP 头⾥⾯的端⼝号，若有socket监听这个端口，就接收
- 绑定 IP 地址的⽬的：⼀台机器是可以有多个⽹卡的，每个⽹卡都有对应的 IP 地址，当绑定⼀个⽹卡时，内核在收到该⽹卡上的包，才会发给我们；

3、绑定完 IP 地址和端⼝后，就可以调⽤ **listen()** 函数进⾏监听，此时对应 TCP 状态图中的 listen，如果我们要判定服务器中⼀个⽹络程序有没有启动，可以通过 netstat 命令查看对应的端⼝号是否有被监听。

- `netstat -anp | grep 3306`

4、服务端进⼊了监听状态后，通过调⽤ **accept()** 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。

5、客户端如何发起连接的？客户端在创建好 Socket 后，调⽤ connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端⼝号，然后的 TCP 三次握⼿就开始了。

> 在TCP的三次连接中，Linux内核会为每个socket维护两个队列
>
> - 半连接队列：还没完成建立连接的队列，都没有完成三次握手，这个队列中，服务端发送了ACK+SYN，处于syn-rcvd的状态
> - 全连接队列：已经建立连接，完成了三次握手，服务端处于estblished的状态

6、当TCP全连接队列不为空后，服务端的**accept()**函数，就会从内核中的TCP全连接队列拿出一个已经完成连接的Socket返回给应用程序，后续传输都用这个Socket

> 监听的 Socket 和真正⽤来传数据的 Socket 是两个：
>
> - 一个是监听Socket
> - 一个是已连接Socket

7、连接建⽴后，客户端和服务端就开始相互传输数据了，双⽅都可以通过 read() 和 write() 函数来读写数据

<img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225100841399.png" alt="image-20220225100841399" style="zoom:50%;" />



### 如何服务更多的用户？

前⾯提到的 TCP Socket 调⽤流程是最简单、最基本的，它基本只能⼀对⼀通信，因为使⽤的是同步阻塞的⽅式，当服务端在还没处理完⼀个客户端的⽹络 I/O 时，或者 读写操作发⽣阻塞时，其他客户端是⽆法与服务端连接的。

> C10K,单机同时处理1万并发



### 多进程模型

基于最原始的阻塞⽹络 I/O， 如果服务器要⽀持多个客户端，其中⽐较传统的⽅式，就是使⽤多进程模型，也就是为每个客户端分配⼀个进程来处理请求。

服务器的主进程负责监听客户的连接，⼀旦与客户端连接完成，accept() 函数就会返回⼀个「已连接Socket」，这时就通过 fork() 函数创建⼀个⼦进程，实际上就把⽗进程所有相关的东⻄都复制⼀份，包括⽂件描述符、内存地址空间、程序计数器、执⾏的代码等。

这两个进程刚复制完的时候，⼏乎⼀摸⼀样。不过，会根据返回值来区分是⽗进程还是⼦进程，如果返回值是 0，则是⼦进程；如果返回值是其他的整数，就是⽗进程。

正因为⼦进程会复制⽗进程的⽂件描述符，于是就可以直接使⽤「已连接 Socket 」和客户端通信了，

可以发现，⼦进程不需要关⼼「监听 Socket」，只需要关⼼「已连接 Socket」；⽗进程则相反，将客户
服务交给⼦进程来处理，因此⽗进程不需要关⼼「已连接 Socket」，只需要关⼼「监听 Socket」。

<img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225102345633.png" alt="image-20220225102345633" style="zoom:80%;" />

这种⽤多个进程来应付多个客户端的⽅式，在应对 100 个客户端还是可⾏的，但是当客户端数量⾼达⼀万时，肯定扛不住的，因为每产⽣⼀个进程，必会占据⼀定的系统资源，⽽且进程间上下⽂切换的“包袱”是很重的，性能会⼤打折扣。
进程的上下⽂切换不仅包含了虚拟内存、栈、全局变量等⽤户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。



### 多线程模型

如果每来⼀个连接就创建⼀个线程，线程运⾏完后，还得操作系统还得销毁线程，虽说线程切换的上写⽂开销不⼤，但是如果频繁创建和销毁线程，系统开销也是不⼩的。

那么，我们可以使⽤**线程池的⽅式**来避免线程的频繁创建和销毁，所谓的线程池，就是提前创建若⼲个线程，这样当由新连接建⽴时，将这个已连接的 Socket 放⼊到⼀个队列⾥，然后线程池⾥的线程负责从队列中取出已连接 Socket 进程处理。

![image-20220225102658523](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225102658523.png)

需要注意的是，这个队列是全局的，每个线程都会操作，为了避免多线程竞争，线程在操作这个队列前要加锁。（生产者消费者模式）



### I/O多路复用

select/poll/epoll 内核提供给⽤户态的多路复⽤系统调⽤，进程可以**通过⼀个系统调⽤函数从内核中获取多个事件**。

> I/O多路复用：一个进程维护多个Socket
>
> 文件描述符fd是内核为了高效管理已被打开的文件所创建的索引，用于指代被打开的文件，对文件所有IO操作相关的系统调用都通过它。linux一切皆文件。

#### select/poll

select实现I/O多路复用的方式，将已连接的Socket放到一个**文件描述符集合**（数组，有上限,默认是1024），然后调用select函数将文件描述符集合拷贝到内核里，让内核来检查是否有网络事件的发生，检查的方式就是**遍历（轮询）**文件描述符集合，当检查到有事件产生后，将此Socket**标记为可读可写**，接着再把整个文件描述符拷贝回用户态，然后用户态还需要通过遍历的方式找到可读可写的Socket，再对其处理。

**select的方式，需要两个遍历文件描述符集合，一次是在内核态，一次是在用户态，还会发生两次拷贝文件描述符，先从用户态传入内核态，经过内核态修改后，再传回用户态。**

select 使⽤固定⻓度的 BitsMap，表示⽂件描述符集合，⽽且所⽀持的⽂件描述符的个数是有限制的，在Linux 系统中，由内核中的 FD_SETSIZE 限制， 默认最⼤值为 1024 ，只能监听 0~1023 的⽂件描述符

poll 不再⽤ BitsMap 来存储所关注的⽂件描述符，取⽽代之⽤动态数组，以链表形式来组织，突破了select 的⽂件描述符个数限制，当然还会受到系统⽂件描述符限制。

但是 **poll 和 select 并没有太⼤的本质区别**，都是**使⽤「线性结构」存储进程关注的 Socket 集合**，因此都需要遍历⽂件描述符集合来找到可读或可写的 Socket，时间复杂度为 O(n)，⽽且也需要在⽤户态与内核态之间拷⻉⽂件描述符集合，这种⽅式随着并发数上来，性能的损耗会呈指数级增⻓。

#### epoll

epoll 通过两个⽅⾯，很好解决了 select/poll 的问题。

一、epoll在**内核中用红黑树**来跟踪进程所有待检测的文件描述符，把需要监控的Socket通过`epoll_ctl()`函数加入到内核中的红黑树里，红黑树是一个高级的数据结构，增删改都是`O(logn)`,通过对这棵树操作，就不需要像select/poll那样每次传入整个Socket集合，只需要传入一个待检测的socket，减少了内核和用户空间来回拷贝

二、**epoll使用事件驱动的机制**，内核里维护了一个链表来记录就绪事件，当某个socket有事件发生，通过回调函数(callback)，内核会将其加入到就绪事件列表，当用户调用`epoll_wait()`，只会返回有事件发生的文件描述符，不需要像select/poll那样轮询整个socket集合

> epoll_wait 实现的内核代码中调⽤了 __put_user 函数，这个函数就是将数据从内核拷⻉到⽤户空间。

<img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225104026751.png" alt="image-20220225104026751" style="zoom:80%;" />





#### epoll高效的原因

https://www.zhihu.com/question/20122137/answer/146866418

##### **第一部分：select和epoll的任务**

**关键词：应用程序 文件句柄 用户态 内核态  监控者**

要比较epoll相比较select高效在什么地方，就需要比较二者做相同事情的方法。

要完成对I/O流的复用需要完成如下几个事情：

1. 用户态怎么将文件句柄传递到内核态？
2. 内核态怎么判断I/O流可读可写？
3. 内核怎么通知监控者有I/O流可读可写？
4. 监控者如何找到可读可写的I/O流并传递给用户态应用程序？
5. 继续循环时监控者怎样重复上述步骤？

搞清楚上述的步骤也就能解开epoll高效的原因了。



**select的做法：**

步骤1的解法：select创建3个文件描述符集，并将这些文件描述符拷贝到内核中，这里限制了文件句柄的最大的数量为1024（注意是全部传入---第一次拷贝）；

步骤2的解法：内核针对读缓冲区和写缓冲区来判断是否可读可写,这个动作和select无关；

步骤3的解法：内核在检测到文件句柄可读/可写时就产生中断通知监控者select，select被内核触发之后，就返回可读可写的文件句柄的总数；

步骤4的解法：select会将之前传递给内核的文件句柄再次从内核传到用户态（第2次拷贝），select返回给用户态的只是可读可写的文件句柄总数，再使用FD_ISSET宏函数来检测哪些文件I/O可读可写（遍历）；

步骤5的解法：select对于事件的监控是建立在内核的修改之上的，也就是说经过一次监控之后，内核会修改位，因此再次监控时需要再次从用户态向内核态进行拷贝（第N次拷贝）



**epoll的做法：**

步骤1的解法：首先执行epoll_create在内核专属于epoll的高速cache区，并在该缓冲区建立红黑树和就绪链表，用户态传入的文件句柄将被放到红黑树中（第一次拷贝）。

步骤2的解法：内核针对读缓冲区和写缓冲区来判断是否可读可写，这个动作与epoll无关；

步骤3的解法：epoll_ctl执行add动作时除了将文件句柄放到红黑树上之外，还向内核注册了该文件句柄的**回调函数**，内核在检测到某句柄可读可写时则调用该回调函数，**回调函数将文件句柄放到就绪链表**。

步骤4的解法：epoll_wait只监控就绪链表就可以，如果就绪链表有文件句柄，则表示该文件句柄可读可写，并返回到用户态（少量的拷贝）；

步骤5的解法：由于内核不修改文件句柄的位，因此只需要在第一次传入就可以重复监控，直到使用epoll_ctl删除，否则不需要重新传入，因此无多次拷贝。



简单说：epoll是继承了select/poll的I/O复用的思想，并在二者的基础上从监控IO流、查找I/O事件等角度来提高效率，具体地说就是内核句柄列表、红黑树、就绪list链表来实现的。



##### **第二部分：epoll详解**

先简单回顾下如何使用C库封装的3个epoll系统调用吧。

1. **int** epoll_create(**int** size);  

1. **int** epoll_ctl(**int** epfd, **int** op, **int** fd, **struct** epoll_event *event);  
2. **int** epoll_wait(**int** epfd, **struct** epoll_event *events,**int** maxevents, **int** timeout);  

使用起来很清晰：

A.epoll_create建立一个epoll对象。参数size是内核保证能够正确处理的最大句柄数，多于这个最大数时内核可不保证效果。

B.epoll_ctl可以操作上面建立的epoll，例如，将刚建立的socket加入到epoll中让其监控(**也就是将I/O流放到内核**)，或者把 epoll正在监控的某个socket句柄移出epoll，不再监控它等等。

C.epoll_wait在调用时，在给定的timeout时间内，当在监控的所有句柄中(就绪链表)有事件发生时，就返回用户态的进程（**也就是在内核层面捕获可读写的I/O事件**）。

从上面的调用方式就可以看到epoll比select/poll的优越之处：

因为后者每次调用时都要传递你所要监控的所有socket给select/poll系统调用，这意味着需要将用户态的socket列表copy到内核态，如果以万计的句柄会导致每次都要copy几十几百KB的内存到内核态，非常低效。而我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。

====>*select监控的句柄列表在用户态，每次调用都需要从用户态将句柄列表拷贝到内核态，但是epoll中句柄就是建立在内核中的，这样就减少了内核和用户态的拷贝，高效的原因之一。*

所以，实际上在你调用epoll_create后，内核就已经在内核态开始准备帮你存储要监控的句柄了，每次调用epoll_ctl只是在往内核的数据结构里塞入新的socket句柄。

在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。

epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，这些socket会以红黑树的形式保存在内核cache里，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。

**epoll高效的原因：** 

​     这是由于**我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件.**

​     当**epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效**。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已.

​      那么，这个准备就绪list链表是怎么维护的呢？

​      当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。



**epoll综合的执行过程：** 

​      如此，一棵红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。



**epoll水平触发和边缘触发的实现：**

​     当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表， 最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了，所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。

*====>区别就在于epoll_wait将socket返回到用户态时是否清空就绪链表。*  

##### **第三部分：epoll高效的本质**

**1.减少用户态和内核态之间的文件句柄拷贝；**

**2.减少对可读可写文件句柄的遍历；**



#### ET和LT

epoll支持两种事件触发模式，分别是**边缘触发（Edge-triggered,ET）和水平触发（level-triggered,LT）**

如果使⽤边缘触发模式，I/O 事件发⽣时只会通知⼀次，⽽且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。因此，我们会循环从⽂件描述符读写数据，那么如果⽂件描述符是阻塞的，没有数据可读写时，进程会阻塞在读写函数那⾥，程序就没办法继续往下执⾏。所以，边缘触发模式⼀般和⾮阻塞 I/O 搭配使⽤，程序会⼀直执⾏ I/O 操作，直到系统调⽤（如 read 和write ）返回错误，错误类型为 **EAGAIN 或 EWOULDBLOCK** 。

⼀般来说，边缘触发的效率⽐⽔平触发的效率要⾼，因为边缘触发可以减少 epoll_wait 的系统调⽤次数，系统调⽤也是有⼀定的开销的的，毕竟也存在上下⽂的切换。

**select/poll 只有⽔平触发模式，epoll 默认的触发模式是⽔平触发，但是可以根据应⽤场景设置为边缘触发模式。**

另外，使⽤ I/O 多路复⽤时，最好搭配⾮阻塞 I/O ⼀起使⽤，多路复⽤ API 返回的事件并不⼀定可读写的，如果使⽤阻塞 I/O， 那么在调⽤read/write 时则会发⽣程序阻塞，因此最好搭配⾮阻塞 I/O，以便应对极少数的特殊情况。

> 系统调用：是用户程序和操作系统之间的接口
>
> ⽤户空间的代码只能访问⼀个局部的内存空间，⽽内核空间的代码可以访问所有内存空间。因此，当程序使⽤⽤户空间时，我们常说该程序在⽤户态执⾏，⽽当程序使内核空间时，程序则在内核态执⾏。
>
> 一旦一个应用程序执行系统调用成功，其 CPU 运行状态将发生变化，由用户层的 Ring3 转移至内核层的 Ring 0
>
> - 内核层次的代码为特权指令，只能在 CPU 的 Ring 0 状态下运行
>
> 应⽤程序如果需要进⼊内核空间，就需要通过系统调⽤，下⾯来看看系统调⽤的过程：
>
> <img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225105504414.png" alt="image-20220225105504414" style="zoom:50%;" />
>
> 内核程序执⾏在内核态，⽤户程序执⾏在⽤户态。当应⽤程序使⽤系统调⽤时，会产⽣⼀个中断。发⽣中断后， CPU 会中断当前在执⾏的⽤户程序，转⽽跳转到中断处理程序，也就是开始执⾏内核程序。内核处理完后，主动触发中断，把 CPU 执⾏权限交回给⽤户程序，回到⽤户态继续⼯作。



### **用户空间 & 内核空间**

操作系统为了保护危险指令不被应用程序直接访问，则将虚拟空间划分为内核空间和用户空间。

- 内核空间：是操作系统的核心，它提供操作系统的最基本的功能，是操作系统工作的基础，它负责管理系统的进程、内存、设备驱动程序、文件和网络系统，决定着系统的性能和稳定性。

- 用户空间：非内核应用程序运行在用户空间。用户空间中的代码运行在较低的特权级别上，只能看到允许它们使用的部分系统资源，并且不能使用某些特定的系统功能，也不能直接访问内核空间和硬件设备



### **用户态** **和** **内核态** **切换**

- 内核态: 执行代码具有对底层硬件的完全且不受限制的访问，可以执行任何 CPU 指令并引用任何内存地址。

- 用户态: 只能受限的访问内存, 且不允许访问外围设备。占用 CPU 的能力被剥夺, CPU 资源可以被其他程序获取。

开始所有应用程序运行在用户空间，这个时候它是用户态。当一个任务（进程）执行系统调用而陷入内核代码中执行时，称进程处于内核态。例如，Java 中需要新建一个线程，`new Thread( Runnable ...)` 之后调用 `start()` 方法时, 看Hotspot Linux 的 JVM 源码实现，最终是调`pthread_create` 系统方法来创建的线程，这里会从用户态切换到内核态完成系统资源的分配，线程的创建。

我们常说用户态和内核态之间的切换开销比较大， 有如下几点：

- 保留用户态现场（上下文、寄存器、用户栈等）

- 复制用户态参数，用户栈切到内核栈，进入内核态

- 额外的检查（因为内核代码对用户不信任）

- 执行内核态代码

- 复制内核态代码执行结果，回到用户态

- 恢复用户态现场（上下文、寄存器、用户栈等）

所以，频繁的IO操作会频繁的造成用户态 —> 内核态 —> 用户态的切换，严重影响系统性能。



## 阻塞与非阻塞 I/O VS 同步与异步 I/O

![image-20220225101814497](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225101814497.png)

I/O是分为两个过程的：

1、数据准备过程

2、数据从内核缓冲区拷贝回用户进程缓冲区的过程

阻塞I/O会阻塞在【过程1】和【过程2】，而非阻塞I/O和基于非阻塞I/O的多路复用会阻塞在【过程2】，所以这三个是同步I/O

异步I/O这两个过程都不会阻塞



### 阻塞I/O

阻塞I/O，当应用程序执行`read`,线程会被阻塞，一直等待内核数据准备好，并把数据从内核缓冲区拷贝回应用程序的缓冲区，当拷贝完成，`read`才结束

**阻塞等待的是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程**

<img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225101208694.png" alt="image-20220225101208694" style="zoom:67%;" />

### 非阻塞I/O

非阻塞的`read`请求会在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区,`read`才算完成

<img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225101335242.png" alt="image-20220225101335242" style="zoom: 67%;" />

**这⾥最后⼀次 read 调⽤，获取数据的过程，是⼀个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷⻉到⽤户程序的缓存区这个过程。**

为了解决这种傻乎乎轮询⽅式，于是 I/O 多路复⽤技术就出来了，如 **select、poll，它是通过 I/O 事件分发**，当内核数据准备好时，再以事件通知应⽤程序进⾏操作。这个做法⼤⼤改善了应⽤进程对 CPU 的利⽤率，在没有被通知的情况下，应⽤进程可以使⽤ CPU 做其他
的事情。

下图是使⽤ select I/O 多路复⽤过程。注意， read 获取数据的过程（数据从内核态拷⻉到⽤户态的过程），也是⼀个**同步的过程**，需要等待：

<img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225101547252.png" alt="image-20220225101547252" style="zoom:67%;" />

### 同步I/O

实际上，⽆论是**阻塞 I/O、⾮阻塞 I/O，还是基于⾮阻塞 I/O 的多路复⽤都是同步调⽤**。因为它们在 read调⽤时，内核将数据从内核空间拷⻉到应⽤程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷⻉效率不⾼，read 调⽤就会在这个同步过程中等待⽐较⻓的时间。

**真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷⻉到⽤户态」这两个过程都不⽤等待。**

### 异步I/O

当我们发起 aio_read 之后，就⽴即返回，内核⾃动将数据从内核空间拷⻉到应⽤程序空间，这个拷⻉过程同样是异步的，内核⾃动完成的，和前⾯的同步操作不⼀样，应⽤程序并不需要主动发起拷⻉动作

<img src="https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225101746649.png" alt="image-20220225101746649" style="zoom:67%;" />



## Reator

Reactor模式：基于⾯向对象的思想，对 I/O 多路复⽤作了⼀层封装，让使⽤者不⽤考虑底层⽹络 API 的细节，只需要关注应⽤代码的编写。

Reactor 模式也叫 Dispatcher 模式，即 I/O 多路复⽤监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程。

Reactor 模式主要由 Reactor 和处理资源池这两个核⼼部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

⼀般来说，C 语⾔实现的是「单 Reactor 单进程 」的⽅案，因为 C 语编写完的程序，运⾏后就是⼀个独⽴的进程，不需要在进程中再创建线程。**Redis就是单Reactor单进程的方案**

⽽ Java 语⾔实现的是「单 Reactor 单线程 」的⽅案，因为 Java 程序是跑在 Java 虚拟机这个进程上⾯的，虚拟机中有很多线程，我们写的 Java 程序只是其中的⼀个线程⽽已。

### 单 Reactor 单进程

「单 Reactor 单进程」的⽅案示意图

![image-20220225112443538](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20220225112443538.png)

可以看到进程⾥有 Reactor、Acceptor、Handler 这三个对象：

- Reactor 对象的作⽤是监听和分发事件；(Redis中的I/O多路复用组件和文件事件分派器)
- Acceptor 对象的作⽤是获取连接；（连接应答处理器）
- Handler 对象的作⽤是处理业务；（命令回复处理器，命令请求处理器等...）

对象⾥的 select、accept、read、send 是系统调⽤函数，dispatch 和 「业务处理」是需要完成的操作，其中 dispatch 是分发事件操作。

接下来，介绍下「单 Reactor 单进程」这个⽅案：

- Reactor 对象通过 select （IO 多路复⽤接⼝） 监听事件，收到事件后通过 dispatch 进⾏分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
- 如果是连接建⽴的事件，则交由 Acceptor 对象进⾏处理，Acceptor 对象会通过 accept ⽅法 获取连接，并创建⼀个 Handler 对象来处理后续的响应事件；
- 如果不是连接建⽴事件， 则交由当前连接对应的 Handler 对象来进⾏响应；
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

优点：

- 单 Reactor 单进程的⽅案因为全部⼯作都在同⼀个进程内完成，所以实现起来⽐较简单，不需要考虑进程间通信，也不⽤担⼼多进程竞争

缺点：

- 因为只有⼀个进程，⽆法充分利⽤ 多核 CPU 的性能；
- Handler 对象在业务处理时，整个进程是⽆法处理其他连接的事件的，如果业务处理耗时⽐较⻓，那么就造成响应的延迟；

所以，**单 Reactor 单进程的⽅案不适⽤I/O密集型的场景，只适⽤于业务处理⾮常快速的场景。**

**Redis 是由 C 语⾔实现的，它采⽤的正是「单 Reactor 单进程」的⽅案，因为 Redis 业务处理主要是在内存中完成，操作的速度是很快的，性能瓶颈不在 CPU 上，所以 Redis 对于命令的处理是单进程的⽅案。**