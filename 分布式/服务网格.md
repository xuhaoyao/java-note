# Service Mesh

## 从微服务框架到服务网格

服务框架的构建理念是⼀个持续演进的过程。

小规模系统⼀般直接使用单体模式开发，⼀个服务即是⼀个应用。

随着业务规模的增⻓，单体应用的维护困难、迭代缓慢、扩展性差等问题就会暴露出来，这个时候就需要对单体应⽤进⾏拆解，将整个系统拆解为微服务架构。

而随着微服务架构的演进，整个系统又有可能出现语⾔碎⽚化，协议碎⽚化等问题，业务上也很可能会产⽣⾃定义分流、统⼀安全控制等新需求，这个时候就可以考虑向服务⽹格演进。

## 微服务框架

![image-20241114210351343](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114210351343.png)

**问题1. 语言碎片化**

字节跳动的在线服务编程语⾔⽐较分散，除了常⻅的 Go 和 Python 之外，还有 Node.js , C++ , Java , Rust , R 等语⾔。
我们知道，要实现⼀个完善的微服务框架，需要有服务发现、服务治理（负载均衡、超时控制、熔断、限流、访问控制、并发控制、流量调度）等复杂的功能。

在微服务框架体系下，这些功能需要使⽤每种语⾔实现⼀遍，这将会导致巨⼤的开发成本和维护成本。

**问题2.更新困难**

推动⽤⼾升级框架往往是⼀件⾮常困难的事情，⽤⼾有可能还会持续使⽤你⼀年以前发布的框架代码。

当某个框架版本发现重⼤Bug的时候，要推动相关服务升级到修复后的版本是⼀件⾮常困难的事情。

当需要推动⼀个全链路新功能的时候，譬如⾃定义分流，我必须推动整个链路的上下游全部升级，才能最终把整个功能上线。

假设推动了⼀半业务升级后，⼜发现了⼀个严重的Bug，那将是⼀场噩梦！

**问题3.服务治理消耗巨⼤**

以Python为例，Python是⼀种解释性语⾔，它的性能是向来被⼈诟病的。上述超时控制、熔断、限流等⼀系列服务治理规则⽤python实现⼀遍，资源消耗⾮常⾼。

⼀般地，python服务中，⽤于处理各种服务治理部分的CPU消耗，占总CPU消耗的50%以上。

**问题4.业务侵入性强**

框架向业务程序注⼊了很多与业务⽆关的代码。

业务开发者在分析⾃⼰的服务的时候，可能会发现有很多服务治理相关的并发线程，对于业务的研发和性能优化有很多⼲扰。

## 服务网格

### 设计目标

- 完美兼容微服务框架体系中提供的服务发现、服务治理等⼀系列功能；
- 解决微服务框架体系中存在的上述问题；
- ⽀持包括访问控制与安全审计，更灵活的⾃定义流量调度等新功能。

![image-20241114210808078](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114210808078.png)

![image-20241114210853504](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114210853504.png)

在⾮Mesh框架下，服务请求到达TLB之后，是直接打⼊Service A的，Service A 需要访问Service B，Service B 需要访问数据库的时候，全部都是直接连接。

嵌⼊业务服务的框架需要处理服务发现，服务治理等⼀系列⼯作。

![image-20241114211016187](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114211016187.png)

在Mesh框架下，我们在每个服务的同⼀个容器内部嵌⼊了⼀个proxy，接管了服务所有的出流量和⼊流量。

![image-20241114211036816](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114211036816.png)

ByteMesh由三个部分构成

- 数据⾯(Data Plane)
  - 数据⾯对应图中的 proxy A 、 proxy B 和 API Gateway ;
- 控制⾯(Control Plane)
  - 控制⾯对应图中的 `control plane` ;
- 云原⽣⽀持层(Cloud-Native Support Layer)
  - 云原⽣⽀持层更多地是辅助性的⼯作，譬如云平台融合、组件健康检查、管理proxy的热更新等，在本图中没有显式的表⽰。

**proxy的本质是⼀个高性能的流量代理。proxy接管流量后，服务不再负责服务发现和服务治理，可以更专注于业务逻辑。**

**proxy只是与服务在同⼀个容器共⽣的⼀个进程，它也不知道流量所需要的服务治理规则，也⽆法进⾏服务发现，相关内容都需要询问control plane。**

**control plane根据proxy上报的服务调⽤信息，下发服务发现和服务治理规则，由proxy负责执⾏。**

为了防⽌流量过⼤和增加延迟，在proxy内部有⼀个缓存层，缓存control plane下放的配置信息。

proxy在第⼀次收到某类请求的时候会访问⼀次control plane，并在之后的过程中每隔⼏秒执⾏异步刷新。

云原⽣⽀持层保证整个系统对⽤⼾基本透明，⽤框架开发的⽤⼾只需要在APPEngine上点⼀下开关，即可使⽤ByteMesh。

各个组件的更新由ByteMesh统⼀负责，升级再也不⽤敦促业务做改动。

### 调用例子

- 服务启动，proxy进程⼀起启动，此时proxy中没有任何规则，也不含任何服务发现信息
- 服务准备发送第⼀个请求，它向 proxy 发送第⼀个数据包，声明：本服务是A，要访问服务B，访问request如下。
- proxy 在本地查找 A->B 是否有缓存，如果有，直接使⽤，如果没有，则proxy 向 cp_cache 询问 A->B 的各种规则
- cp_cache 查找⼀下 A->B 是否有缓存，如果有，直接返回给 proxy， 如果没有，则向 cp_core 询问 A->B 的规则
- cp_core 接受 cp_cache 的请求，使⽤ consul 和 DB 组装⼀系列的规则，返回给 cp_cache。
- 最终，proxy拿到规则，执⾏规则。

### 治理规则

![image-20241114211740964](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114211740964.png)

![image-20241114211726251](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114211726251.png)

![image-20241114211848543](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114211848543.png)

- cp_cache位于控制层这边，proxy每次拉取的时候，请求走到了cp_cache这边，若每次都向cp_core查询的话，控制面查数据库的压力较大，而请求是带着hash tag来的，cp_cache只需要比对一下规则的hash tag与请求过来的hash tag是否一致，若不一致的话，才将cp_cache的response返回回去，一致的话，直接返回http状态码304（`Not Modified`）即可
  - **304 Not Modified** 是一种 HTTP 响应状态码，表示客户端请求的资源自上次请求后没有被修改过。

### 设计目标的达成

#### 兼容服务化框架体系中的服务发现、服务治理等⼀系列功能

服务发现：control plane 通过访问consul获得服务发现的节点信息，返回给proxy，决定流量⾛向。

服务治理：将存量服务治理规则导⼊control plane的存储，并修改MS，使MS规则写⼊的时候双写。control plane决定服务发现和服务治理以后，proxy只要相应执⾏即可。

#### 解决语言碎⽚化问题

由于服务治理和服务发现功能都已经转移到ByteMesh的组件来实现，因此框架只要实现很薄的⼀层即可。

只需要对相应的语⾔实现⼀套序列化⽅法，在很低的成本下即可使⽤统⼀的服务发现和服务治理能⼒。

#### 解决更新困难

由于框架只需要实现很薄的⼀层序列化层，逻辑简单可靠，因此框架层⼏乎没有更新的需求。

Mesh的组件，包括proxy和control plane，由ByteMesh负责热更新，业务⽆需感知。

#### 解决协议多样化，难兼容

proxy设定了⼀种TTHeader的协议标准。只要满⾜这个标准，不同的协议都可以进⾏接⼊。

#### 解决服务治理消耗巨大

所有的服务治理执⾏逻辑转移到proxy，⽽proxy是基于开源的Envoy实现，使⽤C++作为开发语⾔，效率⾮常⾼。

如果公司的Python服务都可以迁移到ByteMesh，将会节约很多很多机器资源。

#### 解决业务侵入性强

嵌⼊业务的框架只保留⼀层⾮常薄的序列化层，对业务的侵⼊⼤幅减少。

#### ⽀持包括访问控制与安全审计

control plane可以配置规则，对服务之间的流量进⾏服务授权，决定请求⽅是否有权限访问某个接⼝。

proxy层可以通过绑定token的⽅式，进⾏⾝份鉴定，防⽌流量伪造。

流量都转为proxy之间的流量之后，该部分流量可以使⽤mTLS进⾏加密，防⽌监听。

#### 更灵活的⾃定义流量调度等新功能

由于整个流量轨迹都被ByteMesh控制，因此可以通过proxy向流量中注⼊标识，然后根据标识决定流量调度规则。

以PPE(Product Preview Environment)为例。

在TLB⼊⼝层，会根据选定的did，向对应的流量注⼊PPE标识。

在流量传播的过程中，如果对应的PPE标识在对应的服务有分流的需求，**control plane会下放分流指令，把对应的流量打到指定下游服务**。

⽤这种⽅式，就完成了服务的⾃定义分流调度

> 在测试环境测试时，由于基准的测试环境是公用的，可能不好随意在上面测试自己的代码，因此，通过染色分流，可以拉起一个属于开发者自己的测试环境，注入染色标识，比如说“aaa”,那么流量就会打到"aaa"对应的测试环境，使得开发者可以独享一个测试环境。



### bytemesh带来了哪些收益

![image-20241114213336956](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114213336956.png)

![image-20241114213348554](https://raw.githubusercontent.com/xuhaoyao/images/master/img/image-20241114213348554.png)



### 问题和挑战

**单点故障**

从架构图上可以发现 control plane 成为了全路径强依赖。这给整个系统的稳定性增加了⼀定的隐患，必须对控制⾯施加⼀系列容灾⽅案。

**延迟**

从多数语⾔来看，使⽤ByteMesh之后，CPU的消耗都会出现⼀定的下降。（CPU: Mesh下业务进程+proxy进程 < UnMesh业务进程）

但是延迟会略有上升。毕竟整个调⽤过程中，增加了业务进程到proxy进程，和proxy进程到业务进程两步。

⽬前平均延迟的上升⼀般可以控制在0.5ms以内。

**维护复杂度**

虽然ByteMesh从设计之初就希望对业务透明，但是不可避免地还是要引⼊⼀系列新的内容。这对于运维⼈员也是⼀个不⼩的挑战。